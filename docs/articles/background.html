<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>mplot philosophy • mplot</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/lumen/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">mplot</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Theory
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/background.html">Background and philosophy</a>
    </li>
    <li>
      <a href="../articles/vip.html">Variable inclusion plots</a>
    </li>
    <li>
      <a href="../articles/msp.html">Model stability plots</a>
    </li>
    <li>
      <a href="../articles/af.html">Adaptive fence</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Applications
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/publications.html">Publications</a>
    </li>
    <li>
      <a href="../articles/diabetes.html">Diabetes</a>
    </li>
    <li>
      <a href="../articles/birthweight.html">Birth weight</a>
    </li>
    <li>
      <a href="../articles/artificial.html">Artificial example</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Tips
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/interactive.html">Interactive plots</a>
    </li>
    <li>
      <a href="../articles/timing.html">Timing</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/garthtarr/mplot">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/people.html">
    <span class="fa fa-info-circle fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>mplot philosophy</h1>
            
          </div>

    
    
<div class="contents">
<blockquote>
<p>Background on model selection and the mplot package.</p>
</blockquote>
<p>The methods provided by the <strong>mplot</strong> package rely heavily on various bootstrap techniques to give an indication of the stability of selecting a given model or variable and even though not done here, could be implemented with resampling methods other than the bootstrap, for example cross validation. The <strong>m</strong> in <strong>mplot</strong> stands for model selection/building and we anticipate that in future more graphs and methods will be added to the package to further aid better and more stable building of regression models. The intention is to encourage researchers to engage more closely with the model selection process, allowing them to pair their experience and domain specific knowledge with comprehensive summaries of the relative importance of various statistical models.</p>
<p>Two major challenges in model building are the vast number of models to choose from and the myriad of ways to do so. Standard approaches include stepwise variable selection techniques and more recently the lasso. A common issue with these and other methods is their instability, that is, the tendency for small changes in the data to lead to the selection of different models.</p>
<p>An early and significant contribution to the use of bootstrap model selection is <span class="citation">Shao (1996)</span> who showed that carefully selecting <span class="math inline">\(m\)</span> in an <span class="math inline">\(m\)</span>-out-of-<span class="math inline">\(n\)</span> bootstrap drives the theoretical properties of the model selector. <span class="citation">Müller &amp; Welsh (2005)</span> and <span class="citation">Müller &amp; Welsh (2009)</span> modified and generalised Shao’s <span class="math inline">\(m\)</span>-out-of-<span class="math inline">\(n\)</span> bootstrap model selection method to robust settings, first in linear regression and then in generalised linear models. The bootstrap is also used in regression models that are not yet covered by the <strong>mplot</strong> package, such as mixed models <span class="citation">(e.g. Shang &amp; Cavanaugh, 2008)</span> or partially linear models <span class="citation">(e.g. Müller &amp; Vial, 2009)</span> as well as for the selection of tuning parameters in regularisation methods <span class="citation">(e.g. Park et al., 2014)</span>.</p>
<p>Assume that we have <span class="math inline">\(n\)</span> independent observations <span class="math inline">\(\mathbf{y} = (y_{1},\ldots,y_{n})^{\top}\)</span> and an <span class="math inline">\(n\times p\)</span> full rank design matrix <span class="math inline">\(\mathbf{X}\)</span> whose columns are indexed by <span class="math inline">\(1,\ldots,p\)</span>. Let <span class="math inline">\(\alpha\)</span> denote any subset of <span class="math inline">\(p_{\alpha}\)</span> distinct elements from <span class="math inline">\(\{1,\ldots,p\}\)</span>. Let <span class="math inline">\(\mathbf{X}_{\alpha}\)</span> be the corresponding <span class="math inline">\(n\times p_{\alpha}\)</span> design matrix and <span class="math inline">\(\mathbf{x}_{\alpha i}^{\top}\)</span> denote the <span class="math inline">\(i\)</span>th row of <span class="math inline">\(\mathbf{X}_{\alpha}\)</span>.</p>
The <strong>mplot</strong> package focuses specifically on linear and generalised linear models (GLM). In the context of GLMs, a model <span class="math inline">\(\alpha\)</span> for the relationship between the response <span class="math inline">\(\mathbf{y}\)</span> and the design matrix <span class="math inline">\(\mathbf{X}_{\alpha}\)</span> is specified by
<span class="math display">\[\begin{align}
\mathbb{E}(\mathbf{y}) = h(\mathbf{X}_{\alpha}^{\top}\boldsymbol{\beta}_{\alpha}), \text{ and }\operatorname{var}(\mathbf{y}) = \sigma^{2}v(h(\mathbf{X}_{\alpha}^{\top}\boldsymbol{\beta}_{\alpha})),
\end{align}\]</span>
<p>where <span class="math inline">\(\boldsymbol{\beta}_{\alpha}\)</span> is an unknown <span class="math inline">\(p_{\alpha}\)</span>-vector of regression parameters and <span class="math inline">\(\sigma\)</span> is an unknown scale parameter. Here <span class="math inline">\(\mathbb{E}(\cdot)\)</span> and <span class="math inline">\(\operatorname{var}(\cdot)\)</span> denote the expected value and variance of a random variable, <span class="math inline">\(h\)</span> is the inverse of the usual link function and both <span class="math inline">\(h\)</span> and <span class="math inline">\(v\)</span> are assumed known. When <span class="math inline">\(h\)</span> is the identity and <span class="math inline">\(v(\cdot)=1\)</span>, we recover the standard linear model.</p>
The purpose of model selection is to choose one or more models <span class="math inline">\(\alpha\)</span> from a set of candidate models, which may be the set of all models <span class="math inline">\(\mathcal{A}\)</span> or a reduced model set (obtained, for example, using any initial screening method). Many model selection procedures assess model fit using the generalised information criterion,
<span class="math display">\[\begin{equation}
\textrm{GIC}(\alpha,\lambda) = \hat{Q}(\alpha) + \lambda p_{\alpha}. \label{GIC}
\end{equation}\]</span>
<p>The <span class="math inline">\(\hat{Q}(\alpha)\)</span> component is a measure of <em>description loss</em> or <em>lack of fit</em>, a function that describes how well a model fits the data, for example, the residual sum of squares or <span class="math inline">\(-2~\times~\text{log-likelihood}\)</span>. The number of independent regression model parameters, <span class="math inline">\(p_{\alpha}\)</span>, is a measure of <em>model complexity</em>. The penalty multiplier, <span class="math inline">\(\lambda\)</span>, determines the properties of the model selection criterion <span class="citation">(Murray et al., 2013; Müller &amp; Welsh, 2010)</span>. Special cases, when <span class="math inline">\(\hat{Q}(\alpha)=-2\times\text{log-likelihood}(\alpha)\)</span>, include the AIC with <span class="math inline">\(\lambda=2\)</span>, BIC with <span class="math inline">\(\lambda=\log(n)\)</span> and more generally the generalised information criterion (GIC) with <span class="math inline">\(\lambda\in\mathbb{R}\)</span> <span class="citation">(Konishi &amp; Kitagawa, 1996)</span>.</p>
<p>The <strong>mplot</strong> package currently implements <em>variable inclusion plots</em>, <em>model stability plots</em> and a model selection procedure inspired by the adaptive fence of <span class="citation">Jiang et al. (2008)</span>. Variable inclusion plots were introduced independently by <span class="citation">Müller &amp; Welsh (2010)</span> and <span class="citation">Meinshausen &amp; Bühlmann (2010)</span>. The idea is that the best model is selected over a range of values of the penalty multiplier <span class="math inline">\(\lambda\)</span> and the results are visualised on a plot which shows how often each variable is included in the best model. These types of plots have previously been referred to as stability paths, model selection curves and most recently variable inclusion plots (VIPs) in <span class="citation">Murray et al. (2013)</span>. An alternative to penalising for the number of variables in a model is to assess the fit of models within each model size. This is the approach taken in our model stability plots where searches are performed over a number of bootstrap replications and the best models for each size are tallied. The rationale is that if there exists a <em>correct</em> model of a particular model size it will be selected overwhelmingly more often than other models of the same size. Finally, the adaptive fence was introduced by <span class="citation">Jiang et al. (2009)</span> to select mixed models. This is the first time code has been made available to implement the adaptive fence and the first time the adaptive fence has been applied to linear and generalised linear models.</p>
<p>For all methods, we provide publication quality classical plot methods using base <strong>R</strong> graphics as well as interactive plots using the <strong>googleVis</strong> package <span class="citation">(Gesmann &amp; Castillo, 2011)</span>. We also add further utility to these plot methods by packaging the results in a <strong>shiny</strong> web interface which facilitates a high degree of interactivity <span class="citation">(Chang, 2015; Chang et al., 2015)</span>.</p>
<p>In the rejoinder to their least angle regression paper, <span class="citation">Tibshirani et al. (2004)</span> comment,</p>
<blockquote>
<p>“In actual practice, or at least in good actual practice, there is a cycle of activity between the investigator, the statistician and the computer … The statistician examines the output critically, as did several of our commentators, discussing the results with the investigator, who may at this point suggest adding or removing explanatory variables, and so on, and so on.”</p>
</blockquote>
<p>We hope the suite of methods available in the <strong>mplot</strong> package adds valuable information to this cycle of activity between researchers and statisticians. In particular, providing statisticians and researchers alike with a deeper understanding of the relative importance of different models and the variables contained therein.</p>
<p>In the artificial example, we demonstrated a situation where giving the researcher more information in a graphical presentation can lead to choosing the <em>correct</em> model when standard stepwise procedures would have failed.</p>
<p>The diabetes data set suggested the existence of a number of different dominant models at various model sizes which could then be investigated further, for example, statistically using cross validation to determine predictive ability, or in discussion with researchers to see which makes the most practical sense. In contrast, there are no clear models suggested for the birth weight example. The adaptive fence has no peaks, nor is there a clearly dominant model in the model stability plot even though all but one variable are more informative than the added redundant variable in the variable inclusion plot.</p>
<p>While the core of the <strong>mplot</strong> package is built around exhaustive searches, this becomes computationally infeasible as the number of variables grows. We have implemented similar visualisations to model stability plots and variable inclusion plots for <strong>glmnet</strong> which brings the concept of model stability to much larger model sizes, though it will no longer be based around exhaustive searches.</p>
<p>The graphs provided by the <strong>mplot</strong> package are a major contribution. A large amount of information is generated by the various methods and the best way to interpret that information is through effective visualisations. For example, as was be shown in the <a href="diabetes">diabetes example</a>, the path a variable takes through the variable inclusion plot is often more important than the average inclusion probability over the range of penalty values considered. It can also be instructive to observe when there are no peaks in the adaptive fence plot as this indicates that the variability of the log-likelihood is limited and no single model stands apart from the others. Such a relatively flat likelihood over various models would also be seen in the model stability plot where there was no dominant model over the range of model sizes considered.</p>
<p>Although interpretation of the model selection plots provided here is something of an <em>art</em>, this is not something to shy away from. We accept and train young statisticians to interpret qq-plots and residual plots. There is a wealth of information in our plots, particularly the interactive versions enhanced with the shiny interface, that can better inform a researchers’ model selection choice.</p>
<div id="references" class="section level4 unnumbered">
<h4 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h4>
<div id="refs" class="references">
<div id="ref-Chang:2015b">
<p>Chang, W. (2015). <em>shinydashboard: Create dashboards with shiny</em>. <a href="https://cran.r-project.org/package=shinydashboard" class="uri">https://cran.r-project.org/package=shinydashboard</a></p>
</div>
<div id="ref-Chang:2015a">
<p>Chang, W., Cheng, J., Allaire, J., Xie, Y., &amp; McPherson, J. (2015). <em>shiny: Web application framework for R</em>. <a href="http://shiny.rstudio.com" class="uri">http://shiny.rstudio.com</a></p>
</div>
<div id="ref-Gesmann:2011">
<p>Gesmann, M., &amp; Castillo, D. de. (2011). Using the Google visualisation API with R. <em>The R Journal</em>, 3(2), 40–44.</p>
</div>
<div id="ref-Jiang:2009">
<p>Jiang, J., Nguyen, T., &amp; Rao, J. S. (2009). A simplified adaptive fence procedure. <em>Statistics &amp; Probability Letters</em>, 79(5), 625–629. DOI:<a href="https://doi.org/10.1016/j.spl.2008.10.014">10.1016/j.spl.2008.10.014</a></p>
</div>
<div id="ref-Jiang:2008">
<p>Jiang, J., Rao, J. S., Gu, Z., &amp; Nguyen, T. (2008). Fence methods for mixed model selection. <em>The Annals of Statistics</em>, 36(4), 1669–1692. DOI:<a href="https://doi.org/10.1214/07-AOS517">10.1214/07-AOS517</a></p>
</div>
<div id="ref-Konishi:1996">
<p>Konishi, S., &amp; Kitagawa, G. (1996). Generalised information criteria in model selection. <em>Biometrika</em>, 83(4), 875–890. DOI:<a href="https://doi.org/10.1093/biomet/83.4.875">10.1093/biomet/83.4.875</a></p>
</div>
<div id="ref-Meinshausen:2010">
<p>Meinshausen, N., &amp; Bühlmann, P. (2010). Stability selection. <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, 72(4), 417–473. DOI:<a href="https://doi.org/10.1111/j.1467-9868.2010.00740.x">10.1111/j.1467-9868.2010.00740.x</a></p>
</div>
<div id="ref-Murray:2013">
<p>Murray, K., Heritier, S., &amp; Müller, S. (2013). Graphical tools for model selection in generalized linear models. <em>Statistics in Medicine</em>, 32(25), 4438–4451. DOI:<a href="https://doi.org/10.1002/sim.5855">10.1002/sim.5855</a></p>
</div>
<div id="ref-Mueller:2009b">
<p>Müller, S., &amp; Vial, C. (2009). Partially linear model selection by the bootstrap. <em>Australian &amp; New Zealand Journal of Statistics</em>, 51(2), 183–200. DOI:<a href="https://doi.org/10.1111/j.1467-842X.2009.00540.x">10.1111/j.1467-842X.2009.00540.x</a></p>
</div>
<div id="ref-Mueller:2005">
<p>Müller, S., &amp; Welsh, A. H. (2005). Outlier robust model selection in linear regression. <em>Journal of the American Statistical Association</em>, 100(472), 1297–1310. DOI:<a href="https://doi.org/10.1198/016214505000000529">10.1198/016214505000000529</a></p>
</div>
<div id="ref-Mueller:2009">
<p>Müller, S., &amp; Welsh, A. H. (2009). Robust model selection in generalized linear models. <em>Statistica Sinica</em>, 19(3), 1155–1170.</p>
</div>
<div id="ref-Mueller:2010">
<p>Müller, S., &amp; Welsh, A. H. (2010). On model selection curves. <em>International Statistical Review</em>, 78(2), 240–256. DOI:<a href="https://doi.org/10.1111/j.1751-5823.2010.00108.x">10.1111/j.1751-5823.2010.00108.x</a></p>
</div>
<div id="ref-Park:2014">
<p>Park, H., Sakaori, F., &amp; Konishi, S. (2014). Robust sparse regression and tuning parameter selection via the efficient bootstrap information criteria. <em>Journal of Statistical Computation and Simulation</em>, 84(7), 1596–1607. DOI:<a href="https://doi.org/10.1080/00949655.2012.755532">10.1080/00949655.2012.755532</a></p>
</div>
<div id="ref-Shang:2008">
<p>Shang, J., &amp; Cavanaugh, J. E. (2008). Bootstrap variants of the Akaike information criterion for mixed model selection. <em>Computational Statistics &amp; Data Analysis</em>, 52(4), 2004–2021. DOI:<a href="https://doi.org/10.1016/j.csda.2007.06.019">10.1016/j.csda.2007.06.019</a></p>
</div>
<div id="ref-Shao:1996">
<p>Shao, J. (1996). Bootstrap model selection. <em>Journal of the American Statistical Association</em>, 91(434), 655. DOI:<a href="https://doi.org/10.2307/2291661">10.2307/2291661</a></p>
</div>
<div id="ref-Tibshirani:2004">
<p>Tibshirani, R. J., Johnstone, I., Hastie, T., &amp; Efron, B. (2004). Least angle regression. <em>The Annals of Statistics</em>, 32(2), 407–499. DOI:<a href="https://doi.org/10.1214/009053604000000067">10.1214/009053604000000067</a></p>
</div>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Garth Tarr, Samuel Mueller, Alan H Welsh.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
